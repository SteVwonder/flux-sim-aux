#!/usr/bin/env python

import csv
import time
import json
import os
import sys
import argparse
import subprocess
import socket
import syslog

# import psutil
import flux
from flux import core, kvs, jsc
from flux.core.inner import ffi

def load_sched_module(args):
    ''' Load the sched module into the enclosing flux instance '''

    load_cmd = ['flux', 'module', 'load', 'sched', 'in-sim=true']
    if args.sched_plugin:
        load_cmd.append("plugin={}".format(args.sched_plugin))
    if args.rdl:
        load_cmd.append("rdl-conf={}".format(args.rdl))
    if args.verbose:
        load_cmd.append("verbosity={}".format(args.verbose))

    print "Loading sched module: {}".format(load_cmd)
    output = subprocess.check_output(load_cmd)
    if len(output) > 0:
        print output

def load_submit_module(args):
    ''' Load the sched module into the enclosing flux instance '''

    load_cmd = ['flux', 'module', 'load', 'submit',
                'job-csv={}'.format(args.job_trace)]

    print "Loading submit module: {}".format(load_cmd)
    output = subprocess.check_output(load_cmd)
    if len(output) > 0:
        print output

def load_exec_module():
    ''' Loads the sim_exec module into the enclosing flux instance '''

    load_cmd = ['flux', 'module', 'load', 'sim_exec']

    print "Loading exec module: {}".format(load_cmd)
    output = subprocess.check_output(load_cmd)
    if len(output) > 0:
        print output

def load_sim_module():
    ''' Loads the sim module into the enclosing flux instance '''

    load_cmd = ['flux', 'module', 'load', 'sim', 'exit-on-complete=false']
    print "Loading sim module: {}".format(load_cmd)
    output = subprocess.check_output(load_cmd)
    if len(output) > 0:
        print output

def load_modules(args):
    ''' Loads all of the necessary modules into the enclosing flux instance '''

    load_sim_module()
    load_submit_module(args)
    load_exec_module()
    load_sched_module(args)

def print_flux_info():
    ''' Loads the sim module into the enclosing flux instance '''

    print "Hostname: {}".format(socket.gethostname())
    print "Python interpreter: {}".format(sys.executable)
    print "Python search path: {}".format(sys.path)
    getattrs = ['local-uri', 'rank', 'size']
    for attr in getattrs:
        cmd = ['flux', 'getattr', attr]
        output = subprocess.check_output(cmd)
        if len(output) > 0:
            print "Flux {}: {}".format(attr, output)
    cmd = ['flux', 'module', 'list', '--rank=all']
    output = subprocess.check_output(cmd)
    if len(output) > 0:
        print "Flux module info: {}".format(output)
    # p = psutil.Process()
    # print "Flux broker CPU affinity: {}".format(p.cpu_affinity())

def get_lwj_dir(fh, job_id):
    payload_str = json.dumps({'ids': [job_id]})
    response = fh.rpc_send('job.kvspath', payload=payload_str)
    paths = response['paths']
    assert len(paths) == 1, "Length of returned paths == {}, not 1".format(paths)
    return paths[0]

pending_jobs = []
running_jobs = []
completed_jobs = []
def get_jsc_cb(outstream):
    fieldnames = ['id', 'nnodes', 'ntasks', 'starting_time', 'complete_time',
                  'walltime', 'submit_time', 'execution_time']
    writer = csv.DictWriter(outstream, fieldnames)
    writer.writeheader()
    def jsc_cb(jcb_str, arg, errnum):
        fh = arg
        jcb = json.loads(jcb_str)
        nstate = jcb[jsc.JSC_STATE_PAIR][jsc.JSC_STATE_PAIR_NSTATE]
        jobid = jcb['jobid']
        value = jsc.job_num2state(nstate)
        print "JSC Event - jobid: {}, value: {}".format(jobid, value)
        if value == 'submitted':
            pending_jobs.append(jobid)
        elif value == 'running':
            pending_jobs.remove(jobid)
            running_jobs.append(jobid)
        elif value == 'complete':
            print "Job completed: {}".format(jobid)
            running_jobs.remove(jobid)
            completed_jobs.append(jobid)
            jobdir_key = get_lwj_dir(fh, jobid)
            complete_key = '{}.complete_time'.format(jobdir_key)
            print "Looking for kvs entry {}, since job {} completed".format(complete_key, jobid)
            while not kvs.exists(fh, complete_key):
                print "{} kvs entry not found, waiting for it to be created".format(complete_key)
                time.sleep(1)
            job_kvs = kvs.get_dir(fh, jobdir_key)
            rowdict = {}
            for key in fieldnames:
                try:
                    rowdict[key] = job_kvs[key]
                except KeyError:
                    pass
            rowdict['id'] = jobid
            writer.writerow(rowdict)
        if len(completed_jobs) > 0 and len(running_jobs) == 0 and len(pending_jobs) == 0:
            print "All children are dead, exiting"
            fh.reactor_stop(fh.get_reactor())
    return jsc_cb

def event_reactor_proc(fh):
    print "Initial_program: Starting event reactor"
    if fh.reactor_run(fh.get_reactor(), 0) < 0:
        fh.fatal_error("event_reactor_proc", "reactor run failed")
    print "Initial_program: Event reactor exited"

class Tee(object):
    '''
    Allows for printing to a file and flux's dmesg buffer simultaneously
    Modeled after the Unix 'tee' comand
    '''
    def __init__(self, name, mode, buffering=None, flux_handle=None):
        self.file = open(name, mode, buffering=buffering)
        if buffering:
            self.stdout = os.fdopen(sys.stdout.fileno(), 'w', buffering)
        else:
            self.stdout = sys.stdout
        self.flux_handle = flux_handle
        sys.stdout = self
    def __del__(self):
        sys.stdout = self.stdout
        self.file.close()
    def write(self, data):
        self.file.write(data)
        if self.flux_handle:
            new_data = data.strip()
            if len(new_data) > 0:
                self.flux_handle.log(syslog.LOG_DEBUG, new_data)
    def flush(self):
        self.file.flush()

def setup_logging(args, flux_handle=None):
    '''
    Replace sys.stdout with an instance of Tee
    Also set the enclosing broker to write out its logs to a file
    '''
    if args.log_dir:
        filename = os.path.join(args.log_dir, "initprog.out")
        Tee(filename, 'w', buffering=0, flux_handle=flux_handle)
        if flux_handle:
            flux_handle.log_set_appname("init_prog")
    else:
        sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)

    if args.log_dir:
        log_filename = os.path.join(args.log_dir, "broker.out")
        setattr_cmd = ['flux', 'setattr', 'log-filename', log_filename]
        subprocess.check_output(setattr_cmd)

def main():
    print "Argv: ", sys.argv
    parser = argparse.ArgumentParser()
    parser.add_argument('job_trace')
    parser.add_argument('--results', '-o',
                        help="directory to store the results in")
    parser.add_argument('--sched_plugin', help="which sched plugin to use")
    parser.add_argument('--log_dir', help="log stdout to a file (arg = directory)")
    parser.add_argument('--rdl', help="path to the rdl file to pass to sched module")
    parser.add_argument('--verbose', '-v', action='count')
    args = parser.parse_args()

    assert os.path.isfile(args.job_trace)
    if args.results:
        assert os.path.isdir(args.results)

    flux_handle = flux.Flux()
    setup_logging(args, flux_handle)
    print_flux_info()

    print "Registering callback with JSC"
    outfilename = os.path.join(args.results, "jobs.csv")
    with open(outfilename, 'wb', 0) as outfile:
        jsc.notify_status(flux_handle, get_jsc_cb(outfile), flux_handle)
        load_modules(args)
        event_reactor_proc(flux_handle)

if __name__ == "__main__":
    main()
